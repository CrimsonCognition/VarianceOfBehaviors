{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86174fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fundamental tools for the game environment model\n",
    "import numpy as np\n",
    "import multiprocess\n",
    "from multiprocess import Process, Queue, Semaphore, Lock\n",
    "import time\n",
    "import random\n",
    "\n",
    "# for visualations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b444a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.11.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from randomchooser import Chooser\n",
    "from exploresims import simulate_explore, sim_1, demo_1, demo_2\n",
    "import explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f4556",
   "metadata": {},
   "source": [
    "When training a Reinforcement Learning (RL) model using a variation of Temporal Difference Error and discrete actions, it is overwhelmingly common to both emphasize the importance of input/action variance and to control output variance and bias.\n",
    "\n",
    "One is far more likely to encounter discussion, advice, and research pertaining to the controlling of the variance in the outputs of a model. This is the so-called Variance or Bias Tradeoff (VBT). The problem here is of course the repetitive use of the term Variance, but given its statistical significance this is mostly unavoidable.\n",
    "\n",
    "This discussion and its accompanying demonstrations are focused on the other side of the spectrum. The variance of the experiences provided to the model during training. How actions are selected and its impact on model convergence, and potentially the quality of the policy, overall.\n",
    "\n",
    "For simplicity of the demonstrations, we shall focus our considerations to RL models most similar to DQN models utilizing algorithms that mathematically resemble the Epsilon Greedy Strategy and Experience Replay Memory in line with Bellman Equation based policy optimizations (i.e. Deep Reinforcement Learning / Deep Q-Learning).\n",
    "\n",
    "Note this restriction is arbitrary and simply serves to limit the scope of the demonstration, to keep things accessible, apologies to the more advanced reader.\n",
    "\n",
    "Unlike the plethora of sources that discuss the Variance Bias Tradeoff of outputs, there are far less discussions on maximizing or minimizing the variance of actions selected by the model during its exploratory phases. It is quite the norm for texts, tutorials, examples, and the like to simply select a uniform distribution, justify it as being mathematically the highest variance, and therefore the least biased towards any one action type for the model to explore and move on.\n",
    "\n",
    "This is fine in theory, it feels intuitive, however this trend should not go unchallenged simply for its face value.\n",
    "\n",
    "The reasoning here is that we want high variance in the random actions selected during exploration. The point is that we want the model, for a time, to make entirely random moves. These moves should not \"prefer\" any one action and build a stored memory of experiences. Then we slowly allow the model to take its own actions and converge on some strategy. This is the fundamental idea behind the Epsilon Greedy strategy.\n",
    "\n",
    "High variance in the random selection ensures a diverse set of experiences and the uniform nature of the distribution prevents bias in examples. This is why the common assumption is so intuitive and is rarely strayed from, or challenged.\n",
    "\n",
    "To avoid this from being an endless drone, lets dip our toes in and  see some demonstrations and other visualizations. To this end I have put together a small game, called Explore. It is like a simple maze game, where an agent, or player, is represented by a blue square and must navigate to the exit or goal, represented by the green square.\n",
    "There may be some number of gray walls obstructing the agent's path, but a path to the goal will always exist.\n",
    "The agent may only choose to move up, down, left, right, or do nothing.\n",
    "\n",
    "For visualization purposes the animation includes a trace or heatmap of the number of times a square has been visited. This matrix is then normalized such that the most visited square is always full white, and unvisited squares are fully black.\n",
    "Please run the next cell and observe this first agent \"play\". I won't mention what algorithm may be at play here, yet, but know the moves are entirely random.\n",
    "10 games will be simulated, up to 80 moves each. The trace will be totaled over all 10 games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc1306",
   "metadata": {},
   "source": [
    "Note these first two demos will not have the walls in play.\n",
    "The windows will hold the result for a few seconds before closing so you can view the final heatmap. Focus on the look and feel of the gameplay you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f728d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = demo_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16828dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now watch this other demo\n",
    "sample_2 = demo_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211140c9",
   "metadata": {},
   "source": [
    "Okay now watch them again, but with the walls present so you can see what the game would be like in general. If an agent gets to the goal, it progresses to the next \"floor\". As in a new map, and goal location is generated. You may have seen the goal move earlier when touched, but now it will include a whole new map. Reaching the goal 10 times is the offical \"win\" condition of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e879c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = demo_1(rebuild=True)\n",
    "t = demo_2(rebuild=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b2d00",
   "metadata": {},
   "source": [
    "Okay so now you have watched two different \"agents\" play a few games. I already told you the first one was random . . . but would you beleive me if I told you the second one was equally as random?\n",
    "\n",
    "Assuming you are infact willing to beleive me, here is a question I want you to ponder.\n",
    "\n",
    "Which one was more random?\n",
    "\n",
    "Feel free to watch them play again if you like but don't get to caught up in trying to guess correctly. The nebulous nature of this randomness is indeed the reason for this demo.\n",
    "\n",
    "Okay here is another question, which heatmap or trace, the thing on the right of the viewing window looks the most variable?\n",
    "\n",
    "Don't worry we saved them earlier so you don't have to keep running the simualtion to look at them. Run the next cell block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cee4235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2946dc96dd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is just some postprocessing for drawing the heat maps\n",
    "heat_1 = sample_1[0][4].copy()\n",
    "heat_2 = sample_2[0][4].copy()\n",
    "\n",
    "# normalizing here to retain relative maxima brightness for each map\n",
    "# imshow also runs a normalize \n",
    "# and would use the max out of both maps to normalize both\n",
    "# If you are interested, you can try commenting them out\n",
    "# but randomness plays a role here as well\n",
    "heat_1 *= 255/heat_1.max()\n",
    "heat_2 *= 255/heat_2.max()\n",
    "\n",
    "# To visually separate the two heat maps\n",
    "spacer = np.zeros((21,3))\n",
    "\n",
    "img = np.hstack([heat_1, spacer, heat_2])\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3043157",
   "metadata": {},
   "source": [
    "Ok so the gotchya here, if you can call it that, is that they are both entirely random. Seriously! \n",
    "\n",
    "In fact they are actually EQUALLY random from a macro perspective of action selection. It may be hard to beleive, depending on how your individual simualtions went but it is indeed true and this becomes the crux of this discussion.\n",
    "\n",
    "Both random decisions processes can be described as uniformily random, however they do not portray the same BEHAVIOR in real time. So what about that second question I posed earlier?\n",
    "\n",
    "If nothing went to statistically wild for your sim, there should be a noticable difference between those two heatmaps above. Namely in the distribution of traveled tiles.\n",
    "\n",
    "The left map uses the standard \"intuitive\" approach and should look more consistent and clustered around the center.\n",
    "\n",
    "The right map uses a single variant of the process I will be explaining in a moment. It should be considerably more chaotic. Fewer bright squares, but a wider spread of tiles \"reached\" or lit up at all.\n",
    "\n",
    "Any tile that is fully black, except the ones in the spacer region, have simply never been traveled to in the sample games played. Ideally you should be able to see far more explored tiles on the extreme boundaries of the map on the right.\n",
    "\n",
    "If that seems not the case for you, feel free to run the first 2 sims again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3968c49",
   "metadata": {},
   "source": [
    "Okay so let me verify for you that these two things truly do look the same from a variance perspective for the choosing of random actions. the object we will be using is simply called a Chooser. If simply passed an integer, it will randomly choose from a discrete action space of that size uniformily.\n",
    "\n",
    "Consider:\n",
    "numpy.random.randint(0, n)\n",
    "\n",
    "It also has the option to be handed a list of ints of size n, and an optional integer value. These are weights, and a duration parameter.\n",
    "\n",
    "The one responsible for the decion making for demo_2 is [0,2,1,0,0] and 6.\n",
    "\n",
    "So creating the choosers looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9924db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular = Chooser(5)\n",
    "other = Chooser(5, [0,2,1,0,0], 6)\n",
    "# Note their action spaces are the same as that is dictated by the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca5339",
   "metadata": {},
   "source": [
    "So now we can quickly simulate a few thousand action choices, and look at the bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67c9c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_counts = np.zeros(5, dtype=\"int32\")\n",
    "other_counts = np.zeros(5, dtype=\"int32\")\n",
    "\n",
    "num_samps = 100 * 1000\n",
    "\n",
    "for i in range(num_samps):  # We are counting each time a certaina action is returned\n",
    "    reg_counts[regular.get_random_action()] += 1\n",
    "    other_counts[other.get_random_action()] += 1\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "choice = np.random.randint(0,2)\n",
    "sns.barplot(x=np.arange(5), y=reg_counts, ax=axs[choice])\n",
    "sns.barplot(x=np.arange(5), y=other_counts, ax=axs[~choice])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e29ba",
   "metadata": {},
   "source": [
    "Yes, you read that code correctly, we don't know which graph is which. Its a blind test, can you tell by just looking at them which one is the different approach?\n",
    "\n",
    "My guess is you'd be right about 50% of the time. If you are the patient sort, go ahead and run even more samples to really drive home how equivalent they are.\n",
    "\n",
    "SO WHAT GIVES??!! Why are they so different if their variance is the same? And given that they are different, is one better than the other?\n",
    "\n",
    "Well this is where I'd like to start defining a few new terms, and where the word \"variance\" is about to get used, a lot.\n",
    "\n",
    "Recall that the standard approach is specifically chosen because it maximizes the variance in the selection of actions. Again, because we want our model to unbiasedly explore courses of actions in the begining of the training process. But we are also fundamentally working working with \"time series\" data. Hence the Temporal Difference error and also part of the reason these algorithms are not equal.\n",
    "\n",
    "Consider a random set of moves on an open board, [up, down, down, left, right, up]. There are at most 6! possible paths, not excluding duplicates. Given some starting point A and ending point B, no matter what order the moves are made in, the agent will always end up at point B. This is because vector addition is associative.\n",
    "\n",
    "This should be relatively straight forward, I know, but its important. Because if we say all we care about is the variance in action selection being maximized, we are really saying all we care about is that we started from A and ended in B (Ignoring collisions with walls).\n",
    "But this could not be further form the truth. The various orders of the moves matter to our training. They constitute different paths.\n",
    "\n",
    "So its time for our first term. I'd like to suggest we call this metric we have been discussing Variance of Actions or VoA. We say we want to maximize the VoA and so we choose a uniform probability for action selection to do so.\n",
    "\n",
    "The difference between demo 1 and demo 2 is not in their VoA though. So where is it? Well I would like to suggest we call this a Variance of Behavior. Which is quite a bit harder to cleanly express. But consider the possible paths in the above paragrpahs. Some permutation of that set over time consitutes a path. That path is the apparent \"behavior\" of our agent.\n",
    "\n",
    "To drive this home I'd like to show you another simualtion, with a much less random behavior but still has a uniform distribution of action selection.\n",
    "\n",
    "So using this working terminology here, this new algorithm will have equivalent VoA but a dramtically different VoB. How different? That is hard to say as quantifying behavior is difficult and is why we use Neural networks in the first place. Go ahead an run the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "61dc4130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_animate = Queue()\n",
    "game = explore.ExploreGame(21, False, False, 0)\n",
    "q_animate.put((game.generate_frame(), game.get_trace()))\n",
    "view_args = [q_animate, \"Explore\", 5, 20, 21, 600, 30]\n",
    "viewer = Process(target=explore.launch_viewer, args=view_args)\n",
    "viewer.start()\n",
    "time.sleep(2)\n",
    "\n",
    "pos = np.random.randint(0, 5)\n",
    "length = np.random.randint(1,10)\n",
    "count = 0\n",
    "switched = 0\n",
    "tracking = np.zeros(5)\n",
    "\n",
    "\n",
    "for i in range(30*60):\n",
    "    game.update(pos)\n",
    "    tracking[pos] += 1\n",
    "    q_animate.put((game.generate_frame(), game.get_trace()))\n",
    "    count += 1\n",
    "    if count >= length:\n",
    "        pos = (pos + 1)%5\n",
    "        switched += 1\n",
    "        count = 0\n",
    "    if switched == 5:\n",
    "        pos = np.random.randint(0, 5)\n",
    "        length = np.random.randint(1,11)\n",
    "        count = 0\n",
    "        switched = 0\n",
    "    time.sleep(1/60)\n",
    "q_animate.put(\"kill\")\n",
    "sns.barplot(x = np.arange(5), y=tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d676c4",
   "metadata": {},
   "source": [
    "Your eyes do not deceive you. Again we have another example where we have technically maximized the VoA of our selection process. But this isn't a sleection process we would actually want, right? The actual behavior of the agent is hardly \"random\".\n",
    "\n",
    "The algorithm is actually quite deterministic. It makes a random roll to seed a fully predictable process. In this case taking a \"circular path\", always left handed, and pickinga new random starting direction after each loop.\n",
    "\n",
    "If length is 1, only 20% or 1 out of every 5 moves is actually random, if length is 10 then 1 out of 50 or 2% of moves are random.\n",
    "\n",
    "So if we know these processes are not the same, and we know all 3 algorithms have the same maximized VoA then maybe there is a problem with simply maximizing the VoA and calling it good. Maybe there are some false assumptions or missed considerations. The VoB of the algorithms is different and as mentioned not \"simply\" quantifiable.\n",
    "\n",
    "So how do we quantify which is better?\n",
    "\n",
    "Well I would like to propose that simply maximizing the VoA is nto good enough. What we actually want is to maximize the VoB. Well more specifically we do want to maximize the VoA, but that does not maxmize our benefit from how we choose actions on its own.\n",
    "\n",
    "Recall the two main reasons for maximizing VoA, and thus the naive approach of uniform probability for sleections are:\n",
    " 1. Create and unbiased selection of actions, to not inluence the policy network in any one direction\n",
    " 2. To create a maximum amount of variance in the Agent's Experiences for the most robust exploration\n",
    " \n",
    "The first of these is indeed met by simply maximizing the VoA. By ensuring a generally uniform distribution of moves, we do eliminate a random bias for any one move in the selection process. But it also doesn't eliminate all biases.\n",
    "\n",
    "But it is my proposal that maximizing the VoA does not actually accomplish the second goal. It sounds like it does, but it doesn't. This is where the justifcation is acepted at face value.\n",
    "\n",
    "Lets define a behavior as some unique order of moves in whatever game. The actions correspond to numbers like this:\n",
    " 0 = up\n",
    " 1 = left\n",
    " 2 = down\n",
    " 3 = right\n",
    " 4 = stay still\n",
    " \n",
    " Our circle turning algorithm produces sequencess of moves that then look like:\n",
    " [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 0, 0, 0, . . .] \n",
    " But more generally, where i is the random starting direction for the sequence\n",
    " ([i, i, (i+1), (i+1), (i+2), (i+2), (i+3), (i+3), (i+4), (i+4), i, i,  . . .]) % 5\n",
    " where the repetition is determined by the length value.\n",
    " \n",
    " So more mathematically, the 0 indexed nth term of a sequence with length l and starting term a looks like\n",
    " (a + k//l)%5\n",
    " Recall in python notation // is integer division and % is mod\n",
    " To check ourselves lets plug in values form the first series, a = 0 and l is 3\n",
    " the first term is k = 0\n",
    " k = 0 : (0 + 0//3)%5 = 0\n",
    " k = 3 : (0 + 3//3)%5 = 1\n",
    " k = 11 : (0 +  11//3)%5 = 3\n",
    " k = 15 : (0 + 15//3)%5 = 0\n",
    " Run the cell below if you are not conviced this makes our sequences, and yes this is a more efficent way to implement the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60a26b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, "
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "l = 4\n",
    "for k in range(32):\n",
    "    print((a + k//l)%5, end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489922eb",
   "metadata": {},
   "source": [
    "So if that is a \"circular\" behavior then something like\n",
    "0, 0, 0, 1, 2 ,2, 2, 1, 0, 0 ,0 . . .\n",
    "would be a thin zig zag heading left\n",
    "0,1,2,1,0 . . .\n",
    "Would be even thinner. Hopefully this makes sense, rememeber the sequence means, up, left, down, left, up, left and so on.\n",
    "\n",
    "So our first problem is that a maximized VoA may eliminate bias towards any one specific move, put another way it minimizes the Bias of Actions (BoA). But it doesn't elimante all bias, intrinsicly.\n",
    "\n",
    "Our circle turning algorithm is a prime example. We know the distribution of actions is uniform, but would you say the behavior is unbiased? It strictly makes left handed circular paths. The only variation comes from the radius of the circle and the starting direction. Meaning we actually have a minimal variance in our sequences, and therefore behavior. \n",
    "\n",
    "We are starting to get to the core of the issue here. Now we can discuss if the standard approach gives us the a maximum variation in behavior, and if not, can we do better, and is it worth our time?\n",
    "\n",
    "The naive approach looks like this in code.\n",
    "np.random.choice(np.arange(5))\n",
    "Giving us the options to select 0 - 4 as actions with equal probability of choosing any of those values. So what is the probability of ever seeing any one particular behavior? Can we expect our agent to frequently exhibit a circular path? Or a zig zag?\n",
    "\n",
    "Well what is the expected movement of our agent given this process? Think expected value but for our movement vector. We have a 20% chance * our action vectors [0,1] and [0, -1] for down and up for example. The .2 is equal for all outcomes so our expected movement is the sum of all 5 movement vectors multiplied by .2. But the sum is equal to 0. So statistically, and this is true for every algorithm witha uniform distribution of moves, our agent should go nowhere.\n",
    "\n",
    "You saw this watching the demo of course where you might have noticed the agent in demo_1 tends to \"linger\" wherever it is and periodically may shioft around. That random shift ocurrs soley by luck.\n",
    "\n",
    "The probability of moving 4 tiles away from a given position in 4 moves is (1/5)^4 and the further we wish to go the less likely it becomes. The standard solution's problem is that it allows for immediate reversals of previously made decisions.\n",
    "\n",
    "Allowing reversals by itself is not a bad thing, it is also tied to other behaviors, but the problem is it ALWAYS allows for reversals at an equal probability.\n",
    "\n",
    "So what we need now is a way to measure the quality of various selection algorithms. A way to measure, or at least estimate our VoB, emperically as opposed to observationally.\n",
    "\n",
    "This means we need to discuss briefly about reward functions, and about the variance in our heat maps. The reality here is that depending on the complexity of our environment or model architecture, exactly how to estimate this will fall into the categories of hyperparameter optimization and feature engineering. Ideally this would be roughly optimized before model training really begins but it does become a step of its own. We will go through that in more detail very soon.\n",
    "\n",
    "So in RL it is very common to have a sparse reward function. This means we want reward function that give feedback, rewards, at as few stages as possible. This is in its own regard an attempt to avoid bias. We are in general trying to have the model find an optimal policy because we don't know the optimal policy ourselves. But writing the perfect reward function would require already knowing the correct policy.\n",
    "\n",
    "The solution then is to only give rewards for doing something explicitly bad or good. And preferably as infrequently as possible. Think, only telling the computer its reward when it loses, wins, or draws a game of chess, and maybe when it tries to make an illegal move. If we deviate from this set up then we run the risk of biasing our model.\n",
    "\n",
    "For example if we know the move E4 is a decent move and give a reward for making that move, our model will be pressured into making that move over anyother move that doesn't have a bonus reward. Which then requires us to define rewards for all other moves, proportional in value to E4. And well, we are simply defining the policy at that point.\n",
    "\n",
    "At the same time however this sparse reward can cause a problem in getting the model to ever \"see\" or receive the rewards. A problem will discuss a bit more in a moment.\n",
    "\n",
    "We can also measure the variance in our heat maps from our simulations. Rememeber the heatmap's values are the frequency of the agent being on that square. So we can measure how evenly our agent explored the space. To do this we can use numpy, recall that the variance is just the standard deviation squared so we can compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "29446267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample_1 :  1043.1179427553334\n",
      "Sample_2 :  774.9346626405666\n",
      "Sample_2 / Sample_1 :  0.7429022461195741\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample_1 : \", heat_1.std()**2)\n",
    "print(\"Sample_2 : \", heat_2.std()**2)\n",
    "print(\"Sample_2 / Sample_1 : \", (heat_2.std()/heat_1.std())**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73df8d7",
   "metadata": {},
   "source": [
    "So here I have been telling you we want to maximize variance and now the one I am telling you is better has lower variance? Well I did tell you the word variance was going to get thrown aorund a lot, and that it was going to be confusing.\n",
    "\n",
    "What we need to keep in mind is the distinction about what we are actualy trying to optimize versus what we can measure. If we were to imagine what the heatmap would look like for some ideal selection algorithm, what would it look like?\n",
    "\n",
    "Remeber what we want is the maximum number of different \"expereinces\" for the model to learn from. So one might imagine that the ideal distribution would be UNIFORM. Meaning we want an algorithm that comes as close as possible (maybe) to exploring every square about the same number of times.\n",
    "\n",
    "Of course given the rules of the game this is simply not possible as the player always spawns at the center of the map. So the environment itself is biasing the exploration to the center. and you may be thinking, just change the rules, allow different spawning points, as a solution.\n",
    "\n",
    "While it is true that could help solve some of our problems it also means changing the game. With a simple game like Explore this would be trivial but Explore is intended as our simple case and may not always be a viable option. Consider a linear RPG with key/item requirements to access certain areas or defeat certain enemies. You are now obligated to solve continuity issues to make the idea worth it.\n",
    "\n",
    "This has been a long secontion of text and I don't want to bore you, so lets get some more examples to advance our discussion. Run the next code block to set up some parameters for some sims we will be running. Please read the comments to understand what each one does. and there are some notes below if you encounter any problems.\n",
    "\n",
    "To help run these quite a bit faster I multiprocess our simulations, but Jupyter doesn't play nice, especially with the join() method. So understand, if you go looking at the code, it is not an ideal or proper multiprocessing implementation.\n",
    "\n",
    "If your computer struggles with later sims, reduce max_processors. It can't go below one though. \n",
    "As we start running multiple games at once, I have shrunk the windows. The size of 300 should work on screens with resolutions 1080p or higher, if you run into issues you can make them smaller or bigger as desired. Be warned, larger game board sizes and small windows can make the animation not viewable. If you want to make the screens bigger you can also reduce max_processors. This will make things more visible but slow down the run time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cbe0e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_processors = min(8, max(1, multiprocess.cpu_count()//2)) # the maximum number of sims to run at once\n",
    "# for convienince the value of max_processors will only be set once\n",
    "#Game params\n",
    "game_size = 21 # the width of the game board, 21 is the default\n",
    "use_maze = False # wether or not the map should include walls\n",
    "use_exclusion = False # a rule constraining the spawn location of the goal, default is True\n",
    "use_noise_boundary = 0 # prevents the first step of maze generation from beging, \n",
    "                       # this value's distance from the edge, so 0 is off and 2 is default\n",
    "game_sparsity = .8 # the minimum value of the sparisty of the initial noise pattern for map generation.\n",
    "                   # Smaller means more walls on average\n",
    "\n",
    "game_args = [game_size, use_maze, use_exclusion, use_noise_boundary, game_sparsity]\n",
    "\n",
    "game_window_size = 600 # the height of viewer window. 300 should work for organizing windows on screens 1080p or larger \n",
    "game_window_framerate = 60 # The polling rate of the viewer for new frames to draw\n",
    "\n",
    "window_args = [game_window_size, game_window_framerate]\n",
    "\n",
    "# The simulation speed is going to exponentially increase to save time, this can look chaotic\n",
    "out = sim_1([Chooser(5)], samples=1000, games=10, turns=40, num_process=8, num_column=3, env_args=game_args, window_args=window_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fa0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725a396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fd4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920d231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c72824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choosers = []\n",
    "choosers.append(Chooser(5))\n",
    "if False:\n",
    "    for i in range(3, 13):\n",
    "        choosers.append(Chooser(5, [0,10,7.5,5,0], i))\n",
    "    for i in range(3, 13):\n",
    "        choosers.append(Chooser(5, [10,10,10,10,10], i))\n",
    "    for i in range(3, 13):\n",
    "        choosers.append(Chooser(5, [1,10,10,10,0], i))\n",
    "else:\n",
    "    choosers.append(Chooser(5, [0, 10, 7.5, 5, 0], 6))\n",
    "    choosers.append(Chooser(5, [0, 10, 7.5, 5, 0], 4))\n",
    "    choosers.append(Chooser(5, [0, 10, 7.5, 5, 0], 5))\n",
    "    choosers.append(Chooser(5, [1, 10, 10, 10, 0], 7))\n",
    "len(choosers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19918366",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sim_1(choosers, samples=1000, games=10, turns=40, num_process=8, num_column=3, env_args=game_args, window_args=window_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1f03cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dd38b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(results)):\n",
    "    max_winrate = 0\n",
    "    pos = 0\n",
    "    for i in range(k, len(results)):\n",
    "        if results[i][2][0] > max_winrate:\n",
    "            max_winrate = results[i][2][0]\n",
    "            pos = i\n",
    "    temp = results[k]\n",
    "    results[k] = results[pos]\n",
    "    results[pos] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7effe7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.231\n",
      "0.228\n",
      "0.216\n",
      "0.175\n",
      "0.088\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results)):\n",
    "    print(results[i][2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf74536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "durations = []\n",
    "win_data = []\n",
    "stdev_data = []\n",
    "trace_averages = []\n",
    "for result in results:\n",
    "    weights.append(result[0])\n",
    "    durations.append(result[1])\n",
    "    win_data.append(result[2])\n",
    "    stdev_data.append(result[3])\n",
    "    trace_averages.append(result[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a53d9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 [1, 10, 10, 10, 0] 0.977 0.0326  <-->  0.9444  -  1.01\n",
      "5 [0, 10, 7.5, 5, 0] 0.946 0.0319  <-->  0.9141  -  0.9779\n",
      "4 [0, 10, 7.5, 5, 0] 0.933 0.0302  <-->  0.9028  -  0.9632\n",
      "6 [0, 10, 7.5, 5, 0] 0.895 0.0312  <-->  0.8638  -  0.9262\n",
      "-1 None 0.732 0.0269  <-->  0.7051  -  0.7589\n"
     ]
    }
   ],
   "source": [
    "for (w, d, r) in zip(weights, durations, win_data):\n",
    "    print(d, w, f\"{r[0]:.4}\", f\"{r[2]:.3}\", \" <--> \", f\"{r[0] - r[2]:.4}\", \" - \", f\"{r[0] + r[2]:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f4b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff635c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f4e6cd3a10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = np.hstack([trace_averages[-2], trace_averages[0]])\n",
    "plt.imshow(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce1b9803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4395645161290322"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mins = np.zeros(len(trace_averages))\n",
    "for i in range(len(trace_averages)):\n",
    "    mins[i] = trace_averages[i].min()\n",
    "mins.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a595fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5024933281844916"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mins.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d80f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f75256a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524.6467"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_averages[-1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "660403a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489.7545"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_averages[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 [0, 10, 7.5, 5, 0] 7.37 0.0274  <-->  7.343  -  7.398\n",
    "#4 [0, 10, 7.5, 5, 0] 7.354 0.0276  <-->  7.326  -  7.381\n",
    "#5 [0, 10, 7.5, 5, 0] 7.318 0.0275  <-->  7.291  -  7.346\n",
    "#7 [1, 10, 10, 10, 0] 7.31 0.0277  <-->  7.282  -  7.337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0d8b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_averages = []\n",
    "win_stds = []\n",
    "win_errors = []\n",
    "for win in win_data:\n",
    "    win_averages.append(win[0])\n",
    "    win_stds.append(win[1])\n",
    "    win_errors.append(win[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4787695",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev_averages = []\n",
    "stdev_stds = []\n",
    "stdev_errors = []\n",
    "for stdev in stdev_data:\n",
    "    stdev_averages.append(stdev[0])\n",
    "    stdev_stds.append(stdev[1])\n",
    "    stdev_errors.append(stdev[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c09a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.915006421778063 0.10708246384246096\n",
      "15.2103472534303 0.08477570267273185\n",
      "16.418986514123485 0.08330403448770658\n",
      "17.957885942316455 0.082570717415026\n",
      "19.480932797157642 0.08758232776463426\n",
      "21.781383165100706 0.09574792182765507\n",
      "15.629925028396283 0.08976795817382\n",
      "13.687060319755307 0.06589713849537174\n",
      "14.736647638912821 0.08364109341070006\n",
      "14.072948662005137 0.08261498998746902\n",
      "13.682836146118154 0.09185016762984498\n",
      "14.127084552989043 0.08014547039543578\n"
     ]
    }
   ],
   "source": [
    "for (mean, error) in zip(stdev_averages, stdev_errors):\n",
    "    print(mean, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867227b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1e4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40434e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765c18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1 = Chooser(5)\n",
    "ch2 = Chooser(5, [1,10,10,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11bc7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol1 = Process(target=simulate_explore, args=[sem, out_lock, output_queue, 100,100,40, ch1, 15, 1.08])\n",
    "lol2 = Process(target=simulate_explore, args=[sem, out_lock, output_queue, 100,100,40, ch2, 15, 1.08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4cc395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol1.start()\n",
    "lol2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ff928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol1.join()\n",
    "lol2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a576950",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "if list:\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddab848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
